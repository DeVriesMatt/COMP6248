{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eeeb7abc468a506",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52980e134e2f9e19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this lab we will implement some of the optimisation methods we learned in the lecture. First, we will start by revisiting gradient descent for linear regression. However, in this implementation we will observe how the model parameters are updated over iterations of the gradient descent algorithm. \n",
    "\n",
    "Let's start by implementing gradient descent on a simple linear regression dataset, like the one you generated in Lab 1, but this time shifted so that it ranges from -5 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-02c39c20df2f2d24",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## generate M data points roughly forming a line (noise added)\n",
    "M = 100\n",
    "theta_true = torch.Tensor([[0.5], [2]])\n",
    "\n",
    "X = 10 * torch.rand(M, 2) - 5\n",
    "X[:, 1] = 1.0\n",
    "\n",
    "y = torch.mm(X, theta_true) + 0.3 * torch.randn(M, 1)\n",
    "\n",
    "## visualise the data by plotting it\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c65a1279417a414",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You should now have data points according to y = mx + b where m = theta_true[0,0] and b = theta_true[1,0]. Note, $m = \\theta_1$ and $b = \\theta_0$.\n",
    "\n",
    "Now, let's implement gradient descent using the Mean Squared Error (MSE) cost function. \n",
    "\n",
    "Recall that: \n",
    "\n",
    "$J(\\theta) = \\frac{1}{2 M} \\sum_{i = 1}^M (h_{\\theta} (x^{(i)}) - y^{(i)} )^2$\n",
    "\n",
    "for $i = 1 \\text{  to iters (or until convergence)}$ <br>\n",
    "\n",
    "$\\hspace{1cm} w_i \\leftarrow w_i - \\eta \\frac{\\partial J}{\\partial w_i}$\n",
    "\n",
    "Implement the functions below in order to plot the cost function as well as the weight updates over iterations of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a52e2455d84ff4da",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-8874a1b99625>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8874a1b99625>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    def grad_cost_func(theta, X, y):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## hypothesis computes $h_theta$\n",
    "def hypothesis(theta, X):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "## grad_cost_func computes the gradient of J for linear regression given J is the MSE \n",
    "def grad_cost_func(theta, X, y): \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "## cost_func computes the cost function J\n",
    "def cost_func(theta, X, y): \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0cb37c31097d491",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now let's plot the updates to see what is happening as we iterate over the algorithm. First, we will plot $J$ as a function of $\\theta_1$ as well as the resulting equation of the line learned over $N=5$ iterations. Once your code is working, modify the value of $\\eta$ to see how it affects convergence.\n",
    "\n",
    "The figure below illustrates what you're aiming to plot. Note, much of the code to generate the figures is given below, you mostly need to complete the 3 functions above and then fill in a few missing lines of code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32de56d64c670852",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<img src=\"Figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59662fa076965410",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### First generate the figure on the left hand side. This plot shows the data and the linear fit of the data as the model parameters change over the 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7663cd58d1e91c37",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Now we can plot the lines over iterations\n",
    "## To do this, we start by constructing a grid of parameter pairs and their corresponding cost function values. \n",
    "x_axis = np.linspace(-1,1,100)\n",
    "theta_grid = torch.Tensor(len(x_axis),2)\n",
    "theta_grid[:,0] = torch.from_numpy(x_axis)\n",
    "theta_grid[:,1] = 2.0\n",
    "\n",
    "J_grid = cost_func(theta_grid.t(), X, y)\n",
    "\n",
    "N = 5\n",
    "eta = 0.03\n",
    "\n",
    "theta_0 = torch.Tensor([[0.0], [2.0]]) #initialise \n",
    "J_t = torch.Tensor(1,N)\n",
    "theta = torch.Tensor(2,1,N)\n",
    "J_t[:,0] = cost_func(theta_0, X, y)[0]\n",
    "theta[:,:,0] = theta_0\n",
    "\n",
    "for j in range(1,N):\n",
    "    last_theta = theta[:,:,j-1]\n",
    "    ## Compute the value of this_theta\n",
    "    ## CODE HERE\n",
    "    theta[:,:,j] = this_theta\n",
    "    J_t[:,j] = cost_func(this_theta,X,y)[0]\n",
    "\n",
    "    \n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "\n",
    "## Plot the data \n",
    "## CODE HERE \n",
    "\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title('Data and fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7fd81fa921c6327",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Next, generate the plots on the right hand side. This figure is a plot of the cost function over the value of $\\theta_1$ as well as the updates of $\\theta_1$ over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c9fa638485b0726e",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## PLOTS HERE\n",
    "\n",
    "# add the plot axes labels and title\n",
    "plt.xlabel(r'$\\theta_1$')\n",
    "plt.ylabel(r'$J(\\theta_1)$')\n",
    "plt.title('Cost function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e70124241b132a09",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Finally, generate a contour plot of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-83e4d7737cbda3d1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Generate a grid of values for theta_0 and theta_1 and compute the cost function for every combination.\n",
    "\n",
    "theta_0_vals = np.linspace(-1.0,1,100)\n",
    "theta_1_vals = np.linspace(-4.0,4,100)\n",
    "theta = torch.Tensor(len(theta_0_vals),2)\n",
    "\n",
    "# Compute the cost function over every combination of values for theta in a variable called J which will then be plot below\n",
    "## CODE HERE\n",
    "\n",
    "xc,yc = np.meshgrid(theta_0_vals, theta_1_vals)\n",
    "contours = plt.contour(xc, yc, J, 20)\n",
    "plt.clabel(contours)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
