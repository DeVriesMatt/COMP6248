{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Stochastic Gradient Descent, Momentum, and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this part of the lab, using the same data and setup as in Part 1, compare how stochastic gradient descent (SGD), SGD with momentum, and adam will perform over iterations. You are free to write all of the code on your own, or if you prefer, you can fill in the missing sections in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by reusing some of the functions you have coded for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## generate M data points roughly forming a line (noise added)\n",
    "M = 50\n",
    "theta_true = torch.Tensor([[0.5], [2]])\n",
    "\n",
    "X = 10 * torch.rand(M, 2) - 5\n",
    "X[:, 1] = 1.0\n",
    "\n",
    "y = torch.mm(X, theta_true) + 0.3 * torch.randn(M, 1)\n",
    "\n",
    "## hypothesis computes $h_theta$\n",
    "def hypothesis(theta, X):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "## grad_cost_func computes the gradient of J for linear regression given J is the MSE \n",
    "def grad_cost_func(theta, X, y): \n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "## cost_func computes\n",
    "def cost_func(theta, X, y):\n",
    "    ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE IS ADAM\n",
    "\n",
    "def weightupdate_adam():\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "def weightupdate_sgd_momentum():\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "def weigthupdate_sgd():\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "N = 200\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "alpha = 0.01\n",
    "\n",
    "theta_0 = torch.Tensor([[2],[4]]) #initialise\n",
    "\n",
    "# Write the code that will call of the optimisation update functions and compute weight updates for each individual data point over N iterations.\n",
    "##Â YOUR CODE HERE\n",
    "\n",
    "theta_0_vals = np.linspace(-2,4,100)\n",
    "theta_1_vals = np.linspace(0,4,100)\n",
    "theta = torch.Tensor(len(theta_0_vals),2)\n",
    "\n",
    "J = np.zeros((len(theta_0_vals),len(theta_1_vals)))\n",
    "     \n",
    "# The following for loops are used to compute the value of the cost function, J, over all the thetas in order to plot the contour below.\n",
    "for i, theta_0 in enumerate(theta_0_vals):\n",
    "    for j, theta_1 in enumerate(theta_1_vals):\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "xc,yc = np.meshgrid(theta_0_vals, theta_1_vals)\n",
    "contours = plt.contour(xc, yc, J, 20)\n",
    "\n",
    "# Now plot the output of SGD, momentum and Adam all on the same plot for comparison\n",
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
