\documentclass{beamer}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{textpos} % package for the positioning

\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\usetheme{Copenhagen}
\hypersetup{pdfstartview={Fit}}
\lstset{basicstyle=\small\ttfamily,breaklines=true}

\title[Differentiation]{The power of differentiation}
\author{Jonathon Hare}
\institute[]
{
  Vision, Learning and Control\\
  University of Southampton 
}
\date{}
\subject{Computer Science}
\useoutertheme{infolines}
\setbeamertemplate{headline}{} %remove headline
\setbeamertemplate{navigation symbols}{} %remove navigation symbols

\begin{document}
  \frame{
  \titlepage
}

\begin{frame}
\frametitle{Topics}
\begin{itemize}
	\item The big idea: optimisation by following gradients
	\item Recap: what are gradients and how do we find them?
	\item Recap: Singular Value Decomposition and its applications
	\item Example: Computing SVD using gradients - The Netflix Challenge
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The big idea: optimisation by following gradients}
\begin{itemize}
	\item<+-> Fundamentally, we're interested in machines that we train by optimising parameters
	\begin{itemize}
		\item<+-> How do we select those parameters?
	\end{itemize}
	\item<+-> In deep learning/differentiable programming we typically define an objective function that we \emph{minimise} (or \emph{maximise}) with respect to those parameters
	\item<+-> This implies that we're looking for points at which the gradient of the objective function is zero w.r.t the parameters
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The big idea: optimisation by following gradients}
\framesubtitle{A simple 1D example}
\end{frame}

\begin{frame}
\frametitle{The big idea: optimisation by following gradients}
\framesubtitle{A simple 2D example}
\end{frame}

\begin{frame}
\frametitle{The big idea: optimisation by following gradients}
\framesubtitle{A more indicative example}
\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{The derivative in 1D}
% \begin{itemize}
	
% \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{The derivative of $y=x^2$ from first principles}
\begin{align*}
    \onslide<+->{y &= x^2 \\}
    \onslide<+->{\frac{dy}{dx} &= \lim_{h\to0} \frac{(x+h)^2 - x^2}{h} \\}
    \onslide<+->{\frac{dy}{dx} &= \lim_{h\to0} \frac{x^2 + h^2 + 2hx - x^2}{h} \\}
    \onslide<+->{\frac{dy}{dx} &= \lim_{h\to0} \frac{h^2 + 2hx}{h} \\}
    \onslide<+->{\frac{dy}{dx} &= \lim_{h\to0} h + 2x) \\}
    \onslide<+->{\frac{dy}{dx} &= 2x}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{Aside: numerical approximation of the derivative}

% central difference as opposed to one sided
\begin{itemize}
\item For numerical computation of gradients it is better to use a "centralised" definition of the gradient:
\begin{itemize}
	\item<+-> $f'(a) = \lim_{h\to0} \frac{f(a+h) - f(a-h)}{2h}$
	\item<+-> This is known as the \emph{symmetric difference quotient}
	\item<+-> For small values of $h$ this has less error than the standard one-sided difference quotient.
\end{itemize}
\item<+-> If you are going to use this to estimate gradients you need to be aware of potential rounding errors due to floating point representations.
\begin{itemize}
	\item Calculating gradients this way using less than 64-bit precision is rarely going to be useful. (Numbers are not represented exactly, so even if $h$ is represented exactly, $x+h$ will probably not be)
	\item You need to pick an appropriate $h$ - too small and the subtraction will have a large rounding error!
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{Derivatives of deeper functions}

\begin{itemize}
	\item<+-> Deep learning is all about optimising deeper functions; functions that are compositions of other functions
	\begin{itemize}
		\item e.g. $z = f \circ g(x) = f(g(x))$
	\end{itemize}
	\item<+-> The chain rule of calculus tells us how to differentiate compositions of functions:
	\begin{itemize}
		\item $\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{x}$
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{Example: differentiating $z=x^4$}

{\small Note that this is a silly example that just serves to demonstrate the principle!}
\begin{align*}
    \onslide<+->{z &= x^4 \\}
    \onslide<+->{z &= (x^2)^2 = y^2 \quad\rm{where}\quad y=x^2 \\}
    \onslide<+->{\frac{dz}{dx} &= \frac{dz}{dy}\frac{dy}{dx} = (2y) (2x) = (2x^2) (2x) = 4x^3}
\end{align*}
\uncover<+->{
Equivalently, from first principles:
\begin{align*}
    z &= x^4 \\
    \frac{dz}{dx} &= \lim_{h\to0} \frac{(x+h)^4 - x^4}{h} \\
    \frac{dz}{dx} &= \lim_{h\to0} \frac{h^4 + 4 h^3 x + 6 h^2 x^2 + 4 h x^3 + x^4 - x^4}{h} \\
    % \onslide<+->{\frac{dz}{dx} &= \lim_{h\to0} \frac{h^4 + 4 h^3 x + 6 h^2 x^2 + 4 h x^3}{h} \\}
    \frac{dz}{dx} &= \lim_{h\to0} h^3 + 4 h^2 x + 6 h x^2 + 4 x^3 = 4 x^3\\
\end{align*}
}
\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{Functions of multiple variables: partial differentiation}


\end{frame}

\begin{frame}
\frametitle{Recap: what are gradients and how do we find them?}
\framesubtitle{Functions of vectors and matrices: partial differentiation}


\end{frame}

\begin{frame}
\frametitle{Recap: Singular Value Decomposition and its applications}
% \begin{itemize}
	
% \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Example: Computing SVD using gradients - The Netflix Challenge}
% \begin{itemize}
	
% \end{itemize}
\end{frame}

\end{document}